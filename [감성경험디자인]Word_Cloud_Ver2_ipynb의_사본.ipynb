{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[감성경험디자인]Word_Cloud_Ver2.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chulminkim86/2019-1.-ManagementAndDatabase/blob/master/%5B%EA%B0%90%EC%84%B1%EA%B2%BD%ED%97%98%EB%94%94%EC%9E%90%EC%9D%B8%5DWord_Cloud_Ver2_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OLSBcKzag1Q"
      },
      "source": [
        "# Ⅰ. 분석에 필요한 라이브러리 설치\n",
        "- 분석에 필요한 라이브러리들을 설치합니다. \n",
        "    - 설치 후 Colab 런타임 재시작이 필요한 라이브러리가 있습니다.\n",
        "    - 설치 후 단순히 import 명령어로 호출하면 되는 라이브러리가 있습니다. \n",
        "    - 이런식으로 편집, 또는 구체적인 의미를 전달할 수 있게 합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcSWXSAFENNL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5agPWv5SR3b"
      },
      "source": [
        "### 첫번째 셀을 실행하고나서 __런타임 - 런타임 다시시작__을 눌러줍니다. (런타임 재시작)  \n",
        "### 런타임이 재시작하고나면 두 번째 셀을 다시 실행합니다. (셀 실행)\n",
        "### 다음 셀부터는 정상적으로 진행하면 됩니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esBLQXrQ-43c"
      },
      "source": [
        "# Colab에 Mecab 설치 (약 6분 소요) # 설치 후 런타임(커널) 재시작 필요 (약 33초)\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "\n",
        "#그래프에 한글 표시하기 #코드 실행 후 런타임(커널) 재시작 필요\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "!apt -qq -y install fonts-nanum\n",
        " \n",
        "import matplotlib.font_manager as fm\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager._rebuild()\n",
        "\n",
        "#한국어 분석을 위한 패키지 설치 \n",
        "!pip install konlpy\n",
        "!pip install pyLDAvis \n",
        "!pip install numpy==1.19.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIOuQ8erRoKs"
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "!apt -qq -y install fonts-nanum\n",
        " \n",
        "import matplotlib.font_manager as fm\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager._rebuild()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u546LiCXStGC"
      },
      "source": [
        "---\n",
        "##### 여기서부터는 런타임 재시작을 하지 않습니다. \n",
        "##### Shift+enter로 셀을 실행시켜보세요. \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCJOZ1bLNcxt"
      },
      "source": [
        "#기본 라이브러리 호출(Import)하기\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#형태소 분석기 Konlpy 불러오기 \n",
        "from konlpy.tag import Twitter; t = Twitter()\n",
        "from konlpy.tag import Mecab\n",
        "twitter = Twitter()\n",
        "\n",
        "#워드클라우드 만들기 \n",
        "from nltk import Text, FreqDist\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from PIL import Image\n",
        "\n",
        "#토픽모델링 라이브러리 사용\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim import corpora, models, similarities, downloader\n",
        "import gensim\n",
        "import pyLDAvis.gensim_models\n",
        "import pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIHXPlcscJMQ"
      },
      "source": [
        "---\n",
        "# Ⅱ. 데이터 분석하기\n",
        "- 우리들은 이제부터 텍스트마이닝을 할 예정입니다. \n",
        "- 가장 먼저 감성분석을 위한 사전학습을 수행할거에요. \n",
        "- 학습 이후 여러분들이 수집한 데이터를 투입하고 분석하도록 하겠습니다.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5kHSWeo5YPJ"
      },
      "source": [
        "##1. (사전학습) 데이터 수집"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGCf6-6oImSy"
      },
      "source": [
        "#사전학습에 사용할 학습데이터를 서버에서 다운받습니다.  \n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\n",
        "\n",
        "#다운받은 학습데이터를 정리하고, 전체 리뷰 개수와 샘플을 살펴봅니다. \n",
        "total_data = pd.read_table('ratings_total.txt', names=['ratings', 'reviews'])\n",
        "print('전체 리뷰 개수 :',len(total_data)) \n",
        "total_data[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icjqQlQG4Zc_"
      },
      "source": [
        "- ratings는 고객이 남긴 별점입니다. 이중 __1점과 2점, 3점은 부정적인 감정__, __4점과 5점은 긍정적인 감정__으로 전제하고 학습하도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki4yUwOq5cgV"
      },
      "source": [
        "## 2. (사전학습) 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cp2S-k0Mk1q"
      },
      "source": [
        "#1점과 2점, 3점에는 0(부정), 4점과 5점은 1(긍정)로 레이블을 붙여줍니다. \n",
        "total_data['label'] = np.select([total_data[\"ratings\"] > 3], [1], default=0)\n",
        "total_data.drop_duplicates(subset=['reviews'], inplace=True) # reviews 열에서 중복인 내용이 있다면 중복 제거\n",
        "\n",
        "#레이블이 붙은 학습 데이터를 사전학습을 위해 트레이닝 데이터와 테스트 데이터로 구분합니다. \n",
        "train_data, test_data = train_test_split(total_data, test_size = 0.25, random_state = 42)\n",
        "\n",
        "# 한글과 공백을 제외하고 모두 제거_traing data\n",
        "train_data['reviews'] = train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
        "train_data['reviews'].replace('', np.nan, inplace=True)\n",
        "\n",
        "# 한글과 공백을 제외하고 모두 제거_test data\n",
        "test_data.drop_duplicates(subset = ['reviews'], inplace=True) # 중복 제거\n",
        "test_data['reviews'] = test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
        "test_data['reviews'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(how='any') # Null 값 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y_TOOJG5rX5"
      },
      "source": [
        "#KoNLpy 중 Mecab tag를 이용해 코퍼스(Courpus)를 만들어줍니다. \n",
        "mecab = Mecab()\n",
        "\n",
        "#불용어 사전을 만들어줍니다. \n",
        "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
        "\n",
        "#트레이닝과 테스트 데이터를 대상으로 각각 불용어를 제거합니다. \n",
        "train_data['tokenized'] = train_data['reviews'].apply(mecab.morphs)\n",
        "train_data['tokenized'] = train_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "test_data['tokenized'] = test_data['reviews'].apply(mecab.morphs)\n",
        "test_data['tokenized'] = test_data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
        "\n",
        "#부정적인 데이터와 긍정적인 데이터를 구분합니다. \n",
        "negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
        "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54NeIKMO_QTS"
      },
      "source": [
        "#3. 사전학습 분류 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lFdfnfQ_XlI"
      },
      "source": [
        "#사전학습 모델에 사용할 데이터를 최종적으로 확정합니다. \n",
        "X_train = train_data['tokenized'].values\n",
        "y_train = train_data['label'].values\n",
        "X_test= test_data['tokenized'].values\n",
        "y_test = test_data['label'].values\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "threshold = 2\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "vocab_size = total_cnt - rare_cnt + 2\n",
        "\n",
        "tokenizer = Tokenizer(vocab_size, oov_token = 'OOV') \n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6aaSj6fA1jx"
      },
      "source": [
        "#사전학습에 사용할 파이썬 라이브러리는 Keras를 호출합니다. \n",
        "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyKuHA-DBi9r"
      },
      "source": [
        "#분류 모델 생성 및 학습 수행\n",
        "X_train = pad_sequences(X_train, maxlen = 80)\n",
        "X_test = pad_sequences(X_test, maxlen = 80)\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v64pbOnvKDmr"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B85k6KrKD_q"
      },
      "source": [
        "## 4. 리뷰의 긍정/부정 감성을 도출하는 함수 __'sentiment_predic'__ 만들기 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPEc2zRcIycJ"
      },
      "source": [
        "#함수 만들기\n",
        "def sentiment_predict(new_sentence):\n",
        "  new_sentence = mecab.morphs(new_sentence) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = 80) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
        "  else:\n",
        "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IuOk67bQc4A"
      },
      "source": [
        "#분석하고 싶은 리뷰를 입력해봅니다. \n",
        "word = input(\"분석하고 싶은 리뷰를 적어주세요: \")\n",
        "sentiment_predict(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN1wynNOtxbW"
      },
      "source": [
        "---\n",
        "#이제부터 각 팀이 수집한 데이터를 투입하고 분석을 해봅시다!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtTKz2m5cQiQ"
      },
      "source": [
        "## 1. 분석할 텍스트 데이터 불러오기\n",
        "- 분석할 데이터를 불러옵니다. \n",
        "    - 여러분들이 수집한 데이터(csv형식)를 사용합니다.   \n",
        "    - 수집한 데이터 이름은 가급적이면 영어로 작성해주세요. \n",
        "    - 수집한 데이터를 업로드 한 후 __파일이름__을 맞춰줘야합니다.   \n",
        "① 왼쪽에 보이는 파일 아이콘![fig1.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABoAAAAXCAIAAACatshHAAAAp0lEQVR42mP8//8/A/UA46hxVDPuzZs3b9++RRaRlZXl4uIixzigWWVlZWgqgMaVl5cTaSKKcRs2bNi0aROmImFhYRERETRBOTm5yMhIwsYR47tv3749fvw4JyfHyMiIgHFA/2poaOA37saNG11dXX5+fgEBAaPGjVDjgIkTmETxG/fo0aPly5cTMA5rJsMDgA5Eyy1YigAgIMYsETBAExxa5d3gMg4At+3O0/AU3GEAAAAASUVORK5CYII=)을 클릭하면 열리는 창에 여러분들의 데이터를 드래그앤드랍(Drag and Drop)으로 복사합니다.   \n",
        "② 런타임 재활용 관련 안내창이 뜨면, 확인을 누릅니다.  \n",
        " ![fig2.png](data:image/png;base64,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)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_zIgfEl2B9J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1701fc97-e89e-471b-d687-a2c003454cbc"
      },
      "source": [
        "#텍스트 데이터를 불러와서 'text'라는 이름의 변수로 저장합니다. \n",
        "#지금은 \"naver.csv\"로 작성되어 있는데, 여러분이 수집한 리뷰 데이터를 사용할 때는 파일 이름을 맞춰줘야 합니다. \n",
        "text = pd.read_csv(\"/content/naver.csv\", encoding='utf-8')\n",
        "\n",
        "#저장된 'text'변수 중 필요한 정보(고객리뷰)만을 골라내 확인해봅니다. \n",
        "#아래 보이는 표에서 리뷰가 적혀 있는 열 이름은 '0.1'입니다. \n",
        "text.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>0</th>\n",
              "      <th>0.1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>19.09.28</td>\n",
              "      <td>💚닥터자르트 시카페어 크림💚(50ml, 48000원)⠀닥터자르트 시카페어 크림은 피...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>19.09.27</td>\n",
              "      <td>화이트&amp;그린 조합으로 깔끔하고 화사한 패키지입니다.지우고 싶은 고민 부위까지 케어하...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>19.09.28</td>\n",
              "      <td>요즘 미세먼지, 환절기, 일교차로 인해 피부가 자극을 받고 극도로 예민해지고 있죠!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>19.09.20</td>\n",
              "      <td>택배 상자 받자마자 포장 때문에 기분이 더 좋았어요!!특히 안쪽에는 시카크림인 만큼...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>21.03.10</td>\n",
              "      <td>우선, 제 피부타입은초초민감성+건성..이고마스크로 인해서 양쪽 볼+턱에여드름이랑 접...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0         0                                                0.1\n",
              "0           0  19.09.28  💚닥터자르트 시카페어 크림💚(50ml, 48000원)⠀닥터자르트 시카페어 크림은 피...\n",
              "1           1  19.09.27  화이트&그린 조합으로 깔끔하고 화사한 패키지입니다.지우고 싶은 고민 부위까지 케어하...\n",
              "2           2  19.09.28  요즘 미세먼지, 환절기, 일교차로 인해 피부가 자극을 받고 극도로 예민해지고 있죠!...\n",
              "3           3  19.09.20  택배 상자 받자마자 포장 때문에 기분이 더 좋았어요!!특히 안쪽에는 시카크림인 만큼...\n",
              "4           4  21.03.10  우선, 제 피부타입은초초민감성+건성..이고마스크로 인해서 양쪽 볼+턱에여드름이랑 접..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIzy4myafMM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d698ea-d260-4482-af01-290fa1d98104"
      },
      "source": [
        "#text뒤에 적혀 있는 ['0.1']은 열 이름입니다. \n",
        "#열 이름을 확인하고, 해당 열 이름을 지정해줍니다. \n",
        "review = text['0.1'] #0.1이 아닌 다른 열이름이라면 해당 열이름을 적어줍니다. \n",
        "review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       💚닥터자르트 시카페어 크림💚(50ml, 48000원)⠀닥터자르트 시카페어 크림은 피...\n",
              "1       화이트&그린 조합으로 깔끔하고 화사한 패키지입니다.지우고 싶은 고민 부위까지 케어하...\n",
              "2       요즘 미세먼지, 환절기, 일교차로 인해 피부가 자극을 받고 극도로 예민해지고 있죠!...\n",
              "3       택배 상자 받자마자 포장 때문에 기분이 더 좋았어요!!특히 안쪽에는 시카크림인 만큼...\n",
              "4       우선, 제 피부타입은초초민감성+건성..이고마스크로 인해서 양쪽 볼+턱에여드름이랑 접...\n",
              "                              ...                        \n",
              "8705                                   싸고 배송도 빠르고 ㅋㅋ대만족^^\n",
              "8706    하리보 젤리는 센스인가요?? ㅎㅎ기분좋게 잘받았습니다 배송도 빠르고 제품도 잘사용하...\n",
              "8707    상호대로 정말 친절하십니다^^ 직원분들도 아주 친절하시구요. 배송도 빨라서 만족합니...\n",
              "8708                       직원분들도 친절하시구 배송도 빠르고 아주 만족해요~^^\n",
              "8709    닥터자르트 시카케어 나온다는 소식을 듣고 일본에서 해외배송 시켜서 잘 바르고 잇어요...\n",
              "Name: 0.1, Length: 8710, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-VENSK8gpdk"
      },
      "source": [
        "##2. 텍스트 데이터 분석을 위한 전처리(preprocesse)수행하기\n",
        "- 수집한 텍스트 데이터는 특수기호나, 오타 때문에 지저분한 상태입니다.\n",
        "- 텍스트 데이터를 분석하기 위해 단어를 숫자화(임베딩, 토크나이징)해야 합니다.\n",
        "- 이 모든 과정을 __텍스트 데이터 전처리__라고 합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ghZ8xS3EX9"
      },
      "source": [
        "#수집된 고객 리뷰 데이터를 파이썬 자료구조인 리스트(list)로 바꿉니다. \n",
        "review_List = review.tolist()\n",
        "\n",
        "#리스트로 바뀐 데이터를 결합해 변수 'review'에 덧씌워줍니다. \n",
        "review_List = '\\n'.join(review_List)\n",
        "\n",
        "#변수 'review'에 저장된 텍스트를 개별 단어(noun)수준으로 쪼개 새로운 변수 'token_text'에 저장합니다. \n",
        "token_text = t.nouns(review_List)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL6ie_YWnDOw"
      },
      "source": [
        "# 3. 텍스트 데이터 상태분석(EDA)\n",
        "- 이제 고객 리뷰데이터는 각 개별단어 수준으로 쪼개져있습니다. \n",
        "- 각 단어들의 빈도 수 등 현황을 살펴보겠습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffeEbnpySumV"
      },
      "source": [
        "#KoNLpy 중 Text tag를 이용해 코퍼스(Courpus)를 만들어줍니다. \n",
        "word_review = Text(token_text)\n",
        "\n",
        "#코퍼스에 포함된 각 단어들의 빈도수를 계산합니다\n",
        "fd_ko = FreqDist(word_review)\n",
        "print(\"총\",fd_ko.N(),\"개 단어가 사용되었습니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLZXI4GRHees"
      },
      "source": [
        "#가장 많이 사용된 상위 10개 단어를 살펴봅니다. \n",
        "fd_ko.most_common(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRurxF2DHhqs"
      },
      "source": [
        "#특정 단어가 몇 번이나 사용되었는지 살펴봅니다. \n",
        "word = input(\"어떤 단어를 찾아볼까요?\")\n",
        "print('{}'.format(fd_ko[word]),'번 사용되었습니다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqsnN-EMLqGw"
      },
      "source": [
        "#사용된 단어 중 상위 50개 단어의 빈도를 그래프로 나타내보겠습니다. \n",
        "plt.figure(figsize=(20, 5)) #그래프 크기를 설정합니다. \n",
        "fd_ko.plot(50) #단어 갯수를 정의합니다. \n",
        "plt.show() #그래프를 만들어줍니다. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikltE7_qrFgZ"
      },
      "source": [
        "생성된 그래프를 보면, 의미없는 단어들이 눈에 보입니다.   \n",
        "이런 단어들을 불용어(不용어)라고 합니다.  \n",
        "불용어는 제거하는 것이 텍스트 분석 성능을 향상시키는데 유리합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Nh2H8_Lz9i"
      },
      "source": [
        "#불용어 사전을 만들어줍니다. \n",
        "stop_words = ['것','거','더','트','때','카','또','늘','저','날','이','제', \"끝\"]\n",
        "\n",
        "#만들어진 불용어 사전에 속한 단어들을 기존 코퍼스에서 제거합니다. \n",
        "clean_word_review = [each_word for each_word in word_review if each_word not in stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u12qjKIXL8Q9"
      },
      "source": [
        "#불용어를 제거한 현황을 살펴봅니다. \n",
        "print(\"불용어 제거 전에는 총\",fd_ko.N(),\"개 단어가 사용되었습니다.\")\n",
        "print(\"불용어 제거 후에는 총\",FreqDist(clean_word_review).N(),\"개 단어가 사용되었습니다.\")\n",
        "print(\"총\", fd_ko.N()-FreqDist(clean_word_review).N(), '개가 제거되었습니다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDSUqQg3yEuI"
      },
      "source": [
        "# 4. 워드클라우드 만들기\n",
        "- 워드클라우드는 많이 사용한 단어를 그림으로 표현한 것입니다. \n",
        "- 워드클라우드를 한번 그려보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB22SE7eMFGp"
      },
      "source": [
        "#워드클라우드 만들기 \n",
        "clean_word_review = Text(clean_word_review)\n",
        "data = clean_word_review.vocab().most_common(200)\n",
        "wordcloud = WordCloud(font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',\n",
        "                     relative_scaling = 0.5,\n",
        "                     background_color='white',\n",
        "                     ).generate_from_frequencies(dict(data))\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmMS9DIfycg1"
      },
      "source": [
        "# Ⅲ. 감성분석(sentiment Analysis) \n",
        "- 감성분석은 사용한 단어에 반영되어 있는 긍/부정 등 감성을 도출하는 분석을 의미합니다. \n",
        "- 단어에 담겨 있는 긍정/부정 감성을 분류하는 다양한 방법이 있지만, 우리들은 네이버 쇼핑몰 리뷰 데이터를 이용해 감성을 __사전학습__하도록 하겠습니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNbZPvTqVvn_"
      },
      "source": [
        "## 5. 수집했었던 데이터의 긍정/부정 감성 분류하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8I3uLucWTtQ"
      },
      "source": [
        "# 만들어둔 함수를 응용해 내가 수집한 댓글 분류하기\n",
        "positive_reviews = [] #긍정\n",
        "negative_reviews = [] #부정\n",
        "\n",
        "def sentiment_classification(sentence):\n",
        "  new_sentence = sentence\n",
        "  new_sentence = mecab.morphs(new_sentence) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = 80) # 패딩\n",
        "  score = float(loaded_model.predict(pad_new)) # 예측\n",
        "  if(score > 0.5):\n",
        "    positive_reviews.append(sentence)\n",
        "  else:\n",
        "    negative_reviews.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-X5fRXkXIix"
      },
      "source": [
        "#분류 작업 수행(약 6분 소요)\n",
        "for i in range(len(review)):\n",
        "    sentiment_classification(review[i])\n",
        "print('긍정적인 리뷰는 총',len(positive_reviews),'개입니다.')\n",
        "print('부정적인 리뷰는 총',len(negative_reviews),'개입니다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF7V5TT5ZP5j"
      },
      "source": [
        "#부정적인 감성 리뷰 출력\n",
        "negative_review = pd.DataFrame(negative_reviews)\n",
        "negative_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPQdrheRAoqC"
      },
      "source": [
        "#긍정적인 감성 리뷰 출력\n",
        "positive_review = pd.DataFrame(positive_reviews)\n",
        "positive_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3BZnFEiAmzy"
      },
      "source": [
        "#각 감성 리뷰 CSV로 저장\n",
        "positive_review.to_csv(\"/content/Positive.csv\", encoding=\"utf-8\")\n",
        "negative_review.to_csv(\"/content/Negative.csv\", encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdCPAmZx4__j"
      },
      "source": [
        "# Ⅳ. 토픽모델링(Topic Modeling) \n",
        "- 감성분석은 주어진 문서(Corpus)를 대상으로 어떤 주제들이 존재하는지 밝혀내는 분석기법입니다. \n",
        "- 전체 문서를 구성하는 각 세부단어들이 보이는 확률분포를 파악하고, 각 토픽들을 발견하게 됩니다.\n",
        "- 분석자는 토픽의 갯수, 구성 단어 갯수 등을 파라미터(Parameter)를 지정해 조정할 수 있으나, 별도의 작업을 통해 최적의 파라미터를 입력하는 것이 일반적입니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0viJ6RMbNtsf"
      },
      "source": [
        "## 1. 사용 데이터 정의\n",
        "- 토픽모델링을 수행할 데이터를 정의합니다. \n",
        "- 지금 우리들은 최초로 입력한 데이터(ex:Naver.csv)에 대한 긍정/부정적 리뷰 데이터를 갖고 있습니다. \n",
        "- 각각 토픽모델링을 수행할 예정이므로 우리는 총 두번의 토픽모델링을 수행합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LsOSnos53gR"
      },
      "source": [
        "#토픽모델링 데이터 정의\n",
        "pos_data = positive_review\n",
        "nega_data = negative_review\n",
        "\n",
        "#각 데이터 컬럼명 변경\n",
        "pos_data.columns = ['reviews']\n",
        "nega_data.columns = ['reviews']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5TB3Tn9O7Hr"
      },
      "source": [
        "## 2. 데이터 전처리\n",
        "- 토픽모델링을 수행하기 위해 데이터 전처리를 수행합니다. \n",
        "- 감성분석에서 이미 한 차례 했지만, 보다 명확한 토픽을 골라내기 위해 중복값 제거 등을 수행합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VqTLjX76jEe"
      },
      "source": [
        "#데이터 프레임의 'review' 열의 값들을 str 형식으로 바꾸기\n",
        "pos_data['reviews'] = pos_data['reviews'].astype(str)\n",
        "nega_data['reviews'] = nega_data['reviews'].astype(str)\n",
        "\n",
        "#review 열을 기준으로 중복된 데이터를 삭제\n",
        "pos_data.drop_duplicates(subset=['reviews'], inplace=True)\n",
        "nega_data.drop_duplicates(subset=['reviews'], inplace=True)\n",
        "\n",
        "#한글이 아니면 빈 문자열로 바꾸기, 정규표현식 사용\n",
        "pos_data['reviews'] = pos_data['reviews'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣]',' ', regex=True)\n",
        "nega_data['reviews'] = nega_data['reviews'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣]',' ', regex=True)\n",
        "\n",
        "#빈 문자열을 널 값(NAN) 값으로 바꾸기\n",
        "pos_data = pos_data.replace({'': np.nan})\n",
        "pos_data = pos_data.replace(r'^\\s*$', None, regex=True)\n",
        "nega_data = nega_data.replace({'': np.nan})\n",
        "nega_data = nega_data.replace(r'^\\s*$', None, regex=True)\n",
        "\n",
        "#널 값(NAN)이 있는 행은 삭제\n",
        "pos_data.dropna(how='any', inplace=True)\n",
        "nega_data.dropna(how='any', inplace=True)\n",
        "\n",
        "#인덱스 재정렬\n",
        "pos_data = pos_data.reset_index (drop = True)\n",
        "nega_data = nega_data.reset_index (drop = True)\n",
        "\n",
        "#텍스트 데이터를 리스트 형태로 변환\n",
        "pos_data_list = pos_data['reviews'].values.tolist()\n",
        "nega_data_list = nega_data['reviews'].values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyWR86QXQCon"
      },
      "source": [
        "#생성된 리스트를 요소별로(댓글 하나) 가져와서 명사만 추출한 후 리스트로 저장\n",
        "pos_data_word=[] \n",
        "nega_data_word=[] \n",
        "\n",
        "mecab = Mecab() #Konlpy 중 okt 사용 (형태소분석기), 향후 다양한 분석기로 실험 \n",
        "\n",
        "#긍정 댓글 처리\n",
        "for i in range(len(pos_data_list)):\n",
        "    try:\n",
        "        pos_data_word.append(mecab.nouns(pos_data_list[i]))\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "#부정 댓글 처리\n",
        "for i in range(len(nega_data_list)):\n",
        "    try:\n",
        "        nega_data_word.append(mecab.nouns(nega_data_list[i]))\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "#만들어진 명사 리스트에서 한글자 단어 삭제\n",
        "pos_dic = []\n",
        "nega_dic = []\n",
        "\n",
        "for n in range(len(pos_data_word)):\n",
        "  for i in range(len(pos_data_word[n])):\n",
        "    if len(pos_data_word[n][i]) >1:\n",
        "      pos_dic.append([pos_data_word[n][i]])\n",
        "\n",
        "for n in range(len(nega_data_word)):\n",
        "  for i in range(len(nega_data_word[n])):\n",
        "    if len(nega_data_word[n][i]) >1:\n",
        "      nega_dic.append([nega_data_word[n][i]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzOg-x2XPOoX"
      },
      "source": [
        "## 3. 토픽모델링을 위한 기본 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXuF600e9Zza"
      },
      "source": [
        "#Word Dictionary 만들기\n",
        "pos_dictionary = corpora.Dictionary(pos_dic)\n",
        "nega_dictionary = corpora.Dictionary(nega_dic)\n",
        "\n",
        "#Corpus 만들기\n",
        "pos_texts = pos_dic\n",
        "nega_texts = nega_dic\n",
        "\n",
        "#(TDF)Term Document Frequency 만들기\n",
        "pos_corpus = [pos_dictionary.doc2bow(text) for text in pos_texts]\n",
        "nega_corpus = [nega_dictionary.doc2bow(text) for text in nega_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4rIjA549h9Y"
      },
      "source": [
        "#LDA model 생성(gensim 사용) 약 1분 소요\n",
        "topic_num = int(input(\"몇개 토픽을 생성할까요? \"))\n",
        "word_num = int(input(\"토픽 당 몇개 단어를 출력할까요? \"))\n",
        "\n",
        "pos_lda_model = gensim.models.ldamodel.LdaModel(corpus=pos_corpus, #활용 Corpus\n",
        "                                            id2word=pos_dictionary, #활용 단어벡터\n",
        "                                            num_topics=topic_num, #생성 토픽수\n",
        "                                            random_state=100,\n",
        "                                            update_every=1,\n",
        "                                            chunksize=100,\n",
        "                                            passes=10,\n",
        "                                            alpha='auto',\n",
        "                                            per_word_topics=True)\n",
        "nega_lda_model = gensim.models.ldamodel.LdaModel(corpus=nega_corpus, #활용 Corpus\n",
        "                                            id2word=nega_dictionary, #활용 단어벡터\n",
        "                                            num_topics=topic_num, #생성 토픽수\n",
        "                                            random_state=100,\n",
        "                                            update_every=1,\n",
        "                                            chunksize=100,\n",
        "                                            passes=10,\n",
        "                                            alpha='auto',\n",
        "                                            per_word_topics=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiJpdZQaLRTB"
      },
      "source": [
        "#긍정적인 리뷰 토픽모델링 결과 Table 생성\n",
        "word_dict = {}\n",
        "num_topic = topic_num\n",
        "for i in range(num_topic):\n",
        "  words = pos_lda_model.show_topic(i, topn = word_num) #topn == 단어개수\n",
        "  word_dict['Pos_Topic #'+'{:02d}'.format(i+1)] = [i[0] for i in words]\n",
        "\n",
        "pd.DataFrame(word_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUxwov33Q9gk"
      },
      "source": [
        "#부정적인 리뷰 토픽모델링 결과 Table 생성\n",
        "word_dict = {}\n",
        "num_topic = topic_num\n",
        "for i in range(num_topic):\n",
        "  words = nega_lda_model.show_topic(i, topn = word_num) #topn == 단어개수\n",
        "  word_dict['Nega_Topic #'+'{:02d}'.format(i+1)] = [i[0] for i in words]\n",
        "\n",
        "pd.DataFrame(word_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqTR8G6V-FQI"
      },
      "source": [
        "#긍정적 토픽모델링 결과 시각화\n",
        "#각 원은 토픽 1개를 의미\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(pos_lda_model, pos_corpus, pos_dictionary) \n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFwzuQhORDrj"
      },
      "source": [
        "#부정적 토픽모델링 결과 시각화\n",
        "#각 원은 토픽 1개를 의미\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(nega_lda_model, nega_corpus, nega_dictionary) \n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}